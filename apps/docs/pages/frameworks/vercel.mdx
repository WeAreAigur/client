import { Callout, Tab, Tabs } from 'nextra-theme-docs'

**All Pipeline Nodes support Vercel Edge Functions out of the box ðŸŽ‰**

You can define them manually and invoke them as usual or read on for the built-in helpers.

## Define Your Edge Function
You can define your Pipeline server endpoints as Edge Functions:
<CH.Code>

```ts /api/joke.ts
import { NextRequest, NextResponse } from 'next/server';
import { jokeGptPipeline } from '#/pipelines/jokegpt'; // <-- import your pipeline

export default async function handler(req: NextRequest) {
  const { input } = input = await req.json(); // <-- receive the input
  const output = await jokeGptPipeline.invoke(input); // <-- invoke the pipeline
  return NextResponse.json(output); // <-- return the output
}

export const config = {
  runtime: 'edge', // <-- define the runtime as edge
};
```
---

```ts client.ts
import { jokeGptPipeline } from '#/pipelines/jokegpt'; // <-- import same pipeline

jokeGptPipeline.invokeRemote('/api/joke', { subject }) // <-- invoke the pipeline
  .then(({ joke }) => {}); 
```
</CH.Code>


## Aigur Vercel Helpers

Aigur Client has built-in helpers for invoking Pipeline on Vercel Edge Functions.

### Define Your Pipeline Repository
Create an object that holds references to your Pipelines where the key is their id.

<CH.Code lineNumbers={true}>
```ts jokegpt.ts focus=2 mark=2[8:14]
export const jokeGptPipeline = aigur.pipeline.create<..., ...>({
  id: 'jokegpt',
  ...
```

---

```ts pipelines/pipelines.ts mark=4[3:9]
import { jokeGptPipeline } from './jokegpt';

export const pipelines = {
  jokegpt: jokeGptPipeline,
} as const;

```
</CH.Code>


### Generic Vercel Edge Function
Next, create a generic Vercel Edge Function that will invoke the Pipeline based on the request path.
You can now invoke any Pipeline and it will automatically run on Vercel Edge Functions (see below how).

<Callout>
	The Generic Edge Function must be defined at /api/pipelines/[id].ts
</Callout>

<CH.Code lineNumbers={true}>
```ts /api/pipelines/[...id].ts
import { NextRequest, NextResponse } from 'next/server';
import { pipelines } from '#/pipelines/pipelines'; // <-- import the pipelines
import { vercelGenericEdge } from '@aigur/client'; // <-- import the helper

export default async function handler(req: NextRequest) {
	const result = await vercelGenericEdge(pipelines, req); // <-- invoke the helper
	if (result instanceof Response) { // <-- the helper function can return error responses
		return result;
	}

	if (result.pipeline.conf.stream) { // <-- support streaming
		return new Response(result.output);
	}
	return NextResponse.json(result.output); // <-- regular output
}

export const config = {
	runtime: 'edge', // <-- define the runtime as edge
};

```
</CH.Code>


### Invoke the Pipeline

We use the `vercel.invoke` helper function to invoke the Pipeline. Behind the scenes it will access our generic Edge Function, invoke the Pipeline and return the result.

<CH.Code>
```ts index.ts mark=1[17:29]
jokeGptPipeline.vercel.invoke({ subject }).then(({text}) => {});
```
</CH.Code>

For streaming using the `vercel.invokeStream` helper function:
<CH.Code>
```ts index.ts mark=2[23:41]
let text = '';
jokeGptStreamPipeline.vercel.invokeStream({ subject }, (res) => {
  text += res;
});
```
</CH.Code>