## Name
`gpt4Prediction`

## Description

Calls GPT4 and returns the result as a string.

## API Key

Uses the `openai` API key:

```ts index.ts
const aigur = createClient({
  apiKeys: {
    openai: process.env.OPENAI_API_KEY
  }
})
```

## Example

```ts index.ts
import { gpt4Prediction } from '@aigur/client';

//...
flow.node(gpt4Prediction, () => ({
  messages: [
    {
      role: 'user',
      content: 'Hello, how are you?'
    }
  ]
})) // --> {text: 'I'm doing well, thank you. How about you?'}
```


## Input

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Type</th>
      <th>Required</th>
      <th>Description</th>
      <th>Default Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>messages</td>
      <td>`Array<{role: 'user' | 'system' | 'assistant', content: string}>`</td>
      <td>Yes</td>
      <td>The prompt to pass GPT3.5-Turbo</td>
      <td></td>
    </tr>
    <tr>
      <td>model</td>
      <td>enum('gpt-4', 'gpt-4-32k')</td>
      <td>No</td>
      <td>ID of the model to use</td>
      <td>gpt-4</td>
    </tr>
    <tr>
      <td>max_tokens</td>
      <td>number</td>
      <td>No</td>
      <td>The maximum number of tokens to generate in the completion.
The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).</td>
      <td>200</td>
    </tr>
    <tr>
      <td>temperature</td>
      <td>number</td>
      <td>No</td>
      <td>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        We generally recommend altering this or `top_p` but not both.</td>
      <td>0.7</td>
    </tr>
    <tr>
      <td>top_p</td>
      <td>number</td>
      <td>No</td>
      <td>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommend altering this or `temperature` but not both. </td> 
      <td>1</td>
    </tr>
    <tr>
      <td>frequency_penalty</td>
      <td>number</td>
      <td>No</td>
      <td>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</td> 
      <td>0</td>
    </tr>
    <tr>
      <td>presence_penalty</td>
      <td>number</td>
      <td>No</td>
      <td>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</td> 
      <td>0</td>
    </tr>
  </tbody>
</table>


## Output

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>text</td>
      <td>string</td>
    </tr>
  </tbody>
</table>
